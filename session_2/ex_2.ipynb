{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "#  Exercise Set 2: Regularization and cross-validation\n",
    "In this exercise set you will learn work with linear regression models to learn more about:\n",
    "- regularization\n",
    "- cross-validation \n",
    "- over/underfitting \n",
    "\n",
    "We import our standard stuff. Notice that we are not interested in seeing the convergence warning in scikit-learn so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling houseprices\n",
    "In this example we will try to predict houseprices using a lot of variable (or features as they are called in Machine Learning). We are going to work with Kaggle's dataset on house prices, see information [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Kaggle is an organization that hosts competitions in building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.0:** Load the california housing data with scikit-learn using the code below. Inspect the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "      <td>10320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.874334</td>\n",
       "      <td>28.506977</td>\n",
       "      <td>5.448060</td>\n",
       "      <td>1.098334</td>\n",
       "      <td>1426.466860</td>\n",
       "      <td>3.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.875069</td>\n",
       "      <td>12.638869</td>\n",
       "      <td>2.710030</td>\n",
       "      <td>0.543761</td>\n",
       "      <td>1098.387561</td>\n",
       "      <td>7.727201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.579425</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.462767</td>\n",
       "      <td>1.006410</td>\n",
       "      <td>787.750000</td>\n",
       "      <td>2.428799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.549850</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.235723</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1162.500000</td>\n",
       "      <td>2.822316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.736450</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.070853</td>\n",
       "      <td>1.098592</td>\n",
       "      <td>1726.250000</td>\n",
       "      <td>3.281516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>16305.000000</td>\n",
       "      <td>599.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  10320.000000  10320.000000  10320.000000  10320.000000  10320.000000   \n",
       "mean       3.874334     28.506977      5.448060      1.098334   1426.466860   \n",
       "std        1.875069     12.638869      2.710030      0.543761   1098.387561   \n",
       "min        0.499900      1.000000      0.888889      0.333333      5.000000   \n",
       "25%        2.579425     18.000000      4.462767      1.006410    787.750000   \n",
       "50%        3.549850     29.000000      5.235723      1.048780   1162.500000   \n",
       "75%        4.736450     37.000000      6.070853      1.098592   1726.250000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  16305.000000   \n",
       "\n",
       "           AveOccup  \n",
       "count  10320.000000  \n",
       "mean       3.046432  \n",
       "std        7.727201  \n",
       "min        1.060606  \n",
       "25%        2.428799  \n",
       "50%        2.822316  \n",
       "75%        3.281516  \n",
       "max      599.714286  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\n",
    "\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex.2.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? \n",
    "\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    "\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10320, 84)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# polynomial transformation\n",
    "poly_trans = PolynomialFeatures(degree=3, include_bias=True)\n",
    "X_train_p = poly_trans.fit_transform(X_train)\n",
    "X_test_p = poly_trans.fit_transform(X_test)\n",
    "\n",
    "print(X_train_p.shape)\n",
    "# X_train_p has 83 features, and 10320 rows. Because features < rows, OLS would not fail.\n",
    "# Note that OLS may become computationally intractable before it fails due \n",
    "# to quadratic scaling in computation time.\n",
    "\n",
    "# rescaling data: we use the distribution of the test data\n",
    "rescaler = StandardScaler().fit(X_train_p)    \n",
    "\n",
    "X_train2 = rescaler.transform(X_train_p)\n",
    "X_test2 = rescaler.transform(X_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.2.2.2**: Estimate the Lasso model on the rescaled train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the rescaled test and train data. \n",
    "\n",
    "> *Hint*: use `logspace` in numpy to create the range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(mse(y_pred, y_true))\n",
    "\n",
    "output = []\n",
    "\n",
    "lambdas =  np.logspace(-4, 4, 20)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "        \n",
    "    reg = Lasso(alpha=lambda_, random_state=1)\n",
    "    reg.fit(X_train2, y_train)\n",
    "    \n",
    " \n",
    "    \n",
    "    output.append([lambda_,\n",
    "                   rmse(reg.predict(X_train2), y_train),\n",
    "                   rmse(reg.predict(X_test2), y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.2.2.3**: Make a plot with on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum RMSE = 0.783 found for lambda = 0.0127.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MSE_df = pd.DataFrame(data=output, \n",
    "                      columns=['lambda', 'MSE train', 'MSE test'])\\\n",
    "           .set_index('lambda')    \n",
    "\n",
    "MSE_df.plot(logx=True, logy=True)\n",
    "\n",
    "# find the minimal observations as a series\n",
    "best_fit = MSE_df['MSE test'].nsmallest(1)\n",
    "\n",
    "# We are interested in minimizing the MSE test\n",
    "# because this tells us \n",
    "# how well out model perform out of sample. \n",
    "\n",
    "# take out the data minimum RMSE and the optimal lambda \n",
    "lambda_opt, RMSE_min = next(best_fit.items()) \n",
    "print('Minimum RMSE = %.3f found for lambda = %.4f.' % (RMSE_min, lambda_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building with pipelines\n",
    "\n",
    "A powerful tool for making and applying models are pipelines, which allows to combine different preprocessing and model procedures into one. This has many advantages, mainly being more safe but also has the added side effect being more code-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.4:** Construct a model building pipeline which \n",
    "\n",
    "> 1. adds polynomial features of degree 3 without bias;\n",
    "> 1. scales the features to mean zero and unit std. \n",
    "\n",
    ">> *Hint:* a modelling pipeline can be constructed with `make_pipeline` from `sklearn.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "pipe_prep = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),                           \n",
    "                          StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and validation\n",
    "\n",
    "\n",
    "### Simple validation\n",
    "In machine learning, we have two types of parameters: those that are learned from\n",
    "the training data, for example, the weights in logistic regression, and the parameters\n",
    "of a learning algorithm that are optimized separately. The latter are the tuning\n",
    "parameters, also called *hyperparameters*, of a model, for example, the regularization\n",
    "parameter in logistic regression or the depth parameter of a decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will regard the \"train\" (aka. development, non-test) data for two purposes. \n",
    "- First we are interested in getting a credible measure of models under different hyperparameters to perform a model selection. \n",
    "- Then with the selected model we estimate/train it on all the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.5:** Make a for loop with 10 iterations where you:\n",
    "1. Split the input data into, train (also know as development) and test where the test sample should be one third. (Set a new random state for each iteration of the loop, so each iteration makes a different split).\n",
    "2. Further split the training (aka development) data into to even sized bins; the first data is for training models and the other is for validating them. (Therefore these data sets are often called training and validation)\n",
    "3. Train a linear regression model with sub-training data. Compute the RMSE for out-of-sample predictions for both the test data  and the validation data. Save the RMSE.\n",
    "\n",
    "> You should now have a 10x2 DataFrame with 10 RMSE from both the test data set and the train data set. Compute descriptive statistics of RMSE for the out-of-sample predictions on test and validation data. Are they simular?    \n",
    ">   They hopefuly are pretty simular. This shows us, that we can split the train data, and use this to fit the model. \n",
    "\n",
    ">> *Hint*: you can reuse code from earlier exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.143827</td>\n",
       "      <td>1.101885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.658022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.777620</td>\n",
       "      <td>0.773043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.781633</td>\n",
       "      <td>0.787416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.794827</td>\n",
       "      <td>0.794032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.800074</td>\n",
       "      <td>0.809688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.899375</td>\n",
       "      <td>2.504750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            test  validation\n",
       "count  10.000000   10.000000\n",
       "mean    1.143827    1.101885\n",
       "std     0.978098    0.658022\n",
       "min     0.777620    0.773043\n",
       "25%     0.781633    0.787416\n",
       "50%     0.794827    0.794032\n",
       "75%     0.800074    0.809688\n",
       "max     3.899375    2.504750"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(mse(y_pred, y_true))\n",
    "\n",
    "output = []\n",
    "\n",
    "for random_state in range(10):\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=random_state)    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=random_state)\n",
    "\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    reg.predict(X_test)\n",
    "\n",
    "    output.append([rmse(reg.predict(X_val), y_val),\n",
    "                   rmse(reg.predict(X_test), y_test)])\n",
    "    \n",
    "pd.DataFrame(output, columns=['test', 'validation']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.6:** Construct a model building pipeline which \n",
    "\n",
    "> 1. adds polynomial features of degree 3 without bias;\n",
    "> 1. scales the features to mean zero and unit std. \n",
    "> 1. estimates a Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),                           \n",
    "                           StandardScaler(),\n",
    "                           Lasso(random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "\n",
    "  \n",
    "The simple validation procedure that we outlined above has one disadvantage: it only uses parts of the *development* data for validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to optimize over both normal parameters and hyperparameteres we do this using nested loops (two-layered cross validation). In outer loop we vary the hyperparameters, and then in the inner loop we do cross validation for the model with the specific selection of hyperparameters. This way we can find the model, with the lowest mean MSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.7:**\n",
    "Run a Lasso regression using the Pipeline from `Ex 2.1.4`. In the outer loop searching through the lambdas specified below. \n",
    "In the inner loop make *5 fold cross validation* on the selected model and store the average MSE for each fold. Which lambda, from the selection below, gives the lowest test MSE?\n",
    " ```python \n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "```\n",
    " *Hint:* `KFold` in `sklearn.model_selection` may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4812941870205839, 0.6266458366460249, 0.0001],\n",
       " [0.48368663031030124, 2.7204993205406627, 0.0005336699231206312],\n",
       " [0.5049716824487993, 0.9900417720540275, 0.002848035868435802],\n",
       " [0.5726973327865992, 0.57963343656599, 0.01519911082952933],\n",
       " [0.6610452974993936, 0.6623662882828436, 0.08111308307896872],\n",
       " [0.88252082505152, 0.883206225220402, 0.43287612810830617],\n",
       " [1.3231104986395466, 1.3232488853041051, 2.310129700083158],\n",
       " [1.3231104986395466, 1.3232488853041051, 12.32846739442066],\n",
       " [1.3231104986395466, 1.3232488853041051, 65.79332246575683],\n",
       " [1.3231104986395466, 1.3232488853041051, 351.11917342151344],\n",
       " [1.3231104986395466, 1.3232488853041051, 1873.8174228603868],\n",
       " [1.3231104986395466, 1.3232488853041051, 10000.0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfolds = KFold(n_splits=5)\n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "mses = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    pipe_lasso = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),\n",
    "                               StandardScaler(),\n",
    "                               Lasso(alpha=lambda_, random_state=1))    \n",
    "    mses_test = []\n",
    "    mses_train = []\n",
    "    \n",
    "    for train_idx, val_idx in kfolds.split(X_dev, y_dev):\n",
    "        \n",
    "        X_train, y_train, = X_dev.iloc[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev.iloc[val_idx], y_dev[val_idx] \n",
    "\n",
    "        pipe_lasso.fit(X_train, y_train)\n",
    "        \n",
    "        mses_train.append(mse(pipe_lasso.predict(X_train), y_train))\n",
    "        mses_test.append(mse(pipe_lasso.predict(X_val), y_val))\n",
    "                \n",
    "    mses.append([sum(mses_train)/len(mses_train),sum(mses_test)/len(mses_test),lambda_])\n",
    "    \n",
    "mses    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for model selection\n",
    "\n",
    "Below we review three useful tools for performing model selection. The first tool, the learning curve, can be used to assess whether there is over- and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.8:** __Automated Cross Validation in one dimension__  \n",
    "Now we want to repeat exercise 12.1.4 in a more automated fasion. \n",
    "When you are doing cross validation with one hyperparameter, you can automate the process by using `validation_curve` from `sklearn.model_selection`. Use this function to search through the values of lambda, and find the value of lambda, which give the lowest test error.  \n",
    "\n",
    "> check if you got the same output for the manual implementation (Ex. 2.1.6) and the automated implementation (Ex. 2.1.7) \n",
    "\n",
    "> BONUS: Plot the average MSE-test and MSE-train against the different values of lambda. (*Hint*: Use logarithmic axes, and lambda as index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "lambdas =  np.logspace(-4, 4, 12)\n",
    "\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator=pipe_lasso,\n",
    "                     X=X_train,\n",
    "                     y=y_train,\n",
    "                     param_name='lasso__alpha',\n",
    "                     param_range=lambdas,\n",
    "                     scoring='neg_mean_squared_error',# scoring='neg_mean_squared_error',                 \n",
    "                     cv=5)\n",
    "\n",
    "mean_values = pd.concat({'train': pd.DataFrame(-train_scores).mean(1), \n",
    "                         'test': pd.DataFrame(-test_scores).mean(1), \n",
    "                         'lambda': pd.DataFrame(lambdas).mean(1)}, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# answer to plotting bonus question\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(mse(y_pred, y_true))\n",
    "\n",
    "\n",
    "# plot the \n",
    "pd.concat({'train': pd.DataFrame(-train_scores).mean(1), \n",
    "           'test': pd.DataFrame(-test_scores).mean(1)},\n",
    "           axis=1)\\\n",
    "    .pipe(np.sqrt)\\\n",
    "    .set_index(pd.Index(lambdas, name='lambda'))\\\n",
    "    .plot(logx=True, logy=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have *more than one* hyperparameter, you will want to fit the model to all the possible combinations of hyperparameters. This is done in an approch called `Grid Search`, which is implementet in `sklearn.model_selection` as `GridSearchCV`\n",
    "\n",
    "> **Ex. 2.1.9:** To get to know `Grid Search` we want to implement in one dimension. Using `GridSearchCV` implement the Lasso, with the same lambdas as before (`lambdas =  np.logspace(-4, 4, 12)`), 10-fold CV and (negative) mean squared error as the scoring variable. Which value of Lambda gives the lowest test error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lasso__alpha': 0.08111308307896872}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_lasso, \n",
    "                  param_grid=[{'lasso__alpha':lambdas}], \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  cv=10, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.10 BONUS** Expand the Lasso pipe from the last excercise with a Principal Component Analisys (PCA), and expand the Grid Search to searching in two dimensions (both along the values of lambda and the values of principal components (n_components)). Is `n_components` a hyperparameter? Which hyperparameters does the Grid Search select as the best?\n",
    "\n",
    "> NB. This might take a while to calculate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lasso__alpha': 0.002848035868435802, 'pca__n_components': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe_sq_pca_lasso = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                                  StandardScaler(),\n",
    "                                  PCA(),\n",
    "                                  Lasso())\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_sq_pca_lasso, \n",
    "                  param_grid=[{'lasso__alpha':lambdas, \n",
    "                               'pca__n_components':range(1, X_train.shape[1]+1)}], \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  cv=10, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "# Yes, n_components is a hyperparameter. \n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.1.11 BONUS** repeat the previous now with RandomizedSearchCV with 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pca__n_components': 6, 'lasso__alpha': 0.002848035868435802}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipe_sq_pca_lasso = make_pipeline(PolynomialFeatures(degree=2,\n",
    "                                                     include_bias=False), \n",
    "                                  StandardScaler(),\n",
    "                                  PCA(),\n",
    "                                  Lasso())\n",
    "\n",
    "\n",
    "gs = RandomizedSearchCV(estimator=pipe_sq_pca_lasso, \n",
    "                        param_distributions=[{'lasso__alpha':lambdas, \n",
    "                                              'pca__n_components':range(1, X_train.shape[1]+1)}], \n",
    "                        scoring='neg_mean_squared_error', \n",
    "                        cv=10, \n",
    "                        n_jobs=-1,\n",
    "                        n_iter=20)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "# Yes, n_components is a hyperparameter. \n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex. 2.1.12 BONUS** read about nested cross validation. How might we implement this in answer 2.1.9?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 2.1.12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
